{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtAWD6DlVki21mJ6jkMqQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avazork/NN/blob/main/tweets_convolution_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начнём с уже знакомой загрузки данных."
      ],
      "metadata": {
        "id": "R2NW41fKTjw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Avazork/NN.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT2k_Gtms_Ew",
        "outputId": "fa9e7f03-c84d-4019-ed60-fb3c4ed65b78"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NN'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 26 (delta 11), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала, конечно, загрузим модель fast text, предобученную для нашего материала."
      ],
      "metadata": {
        "id": "KKnxmufatN1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /root/input/ -c \"http://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.bin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goN5Grt6tXeQ",
        "outputId": "0425187e-e4b2-41a7-da53-09dbeccf07a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-16 05:09:50--  http://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.bin\n",
            "Resolving files.deeppavlov.ai (files.deeppavlov.ai)... 178.63.27.41\n",
            "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|178.63.27.41|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.bin [following]\n",
            "--2023-01-16 05:09:50--  https://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.bin\n",
            "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|178.63.27.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3417475450 (3.2G) [application/octet-stream]\n",
            "Saving to: ‘/root/input/ft_native_300_ru_twitter_nltk_word_tokenize.bin’\n",
            "\n",
            "ft_native_300_ru_tw 100%[===================>]   3.18G  23.2MB/s    in 2m 20s  \n",
            "\n",
            "2023-01-16 05:12:11 (23.2 MB/s) - ‘/root/input/ft_native_300_ru_twitter_nltk_word_tokenize.bin’ saved [3417475450/3417475450]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "fasttext = FastText()\n",
        "fasttext.file_name='/root/input/ft_native_300_ru_twitter_nltk_word_tokenize.bin'\n",
        "fasttext.load_binary_data()"
      ],
      "metadata": {
        "id": "Txi-vQoctbTO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А вот теперь сами данные."
      ],
      "metadata": {
        "id": "UMWsHrSNRSvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "import csv\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "def load_sentirueval_2016(file_name: str) -> Tuple[List[str], List[str]]:\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_name, mode='rb') as fp:\n",
        "        xml_data = fp.read()\n",
        "    root = etree.fromstring(xml_data)\n",
        "    for database in root.getchildren():\n",
        "        if database.tag == 'database':\n",
        "            for table in database.getchildren():\n",
        "                if table.tag != 'table':\n",
        "                    continue\n",
        "                new_text = None\n",
        "                new_label = None\n",
        "                for column in table.getchildren():\n",
        "                    if column.get('name') == 'text':\n",
        "                        new_text = str(column.text).strip()\n",
        "                        if new_label is not None:\n",
        "                            break\n",
        "                    elif column.get('name') not in {'id', 'twitid', 'date'}:\n",
        "                        if new_label is None:\n",
        "                            label_candidate = str(column.text).strip()\n",
        "                            if label_candidate in {'0', '1', '-1'}:\n",
        "                                new_label = 'negative' if label_candidate == '-1' else \\\n",
        "                                    ('positive' if label_candidate == '1' else 'neutral')\n",
        "                                if new_text is not None:\n",
        "                                    break\n",
        "                if (new_text is None) or (new_label is None):\n",
        "                    raise ValueError('File `{0}` contains some error!'.format(file_name))\n",
        "                texts.append(new_text)\n",
        "                labels.append(new_label)\n",
        "            break\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "texts,labels=load_sentirueval_2016('/content/NN/bank_train_2016.xml')\n",
        "label_to_num={'negative':0,'neutral':1,'positive':2}\n",
        "num_labels=[label_to_num[label] for label in labels]\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "RANDOM_SEED = 23\n",
        "random.seed(RANDOM_SEED)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, num_labels, test_size=0.1, random_state=RANDOM_SEED)\n",
        "vocab=[]\n",
        "for text in texts:\n",
        "  tokenized_text=word_tokenize(text)\n",
        "  for word in tokenized_text:\n",
        "    if word not in vocab:\n",
        "      vocab.append(word)\n",
        "embedding_matrix=np.zeros((len(vocab)+1, 100))\n",
        "for i,word in enumerate(vocab):\n",
        "  embedding_matrix[i,0:100]=fasttext[word]\n",
        "max_length=max([len(word_tokenize(text)) for text in texts])\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "encoded_docs_train = tokenizer.texts_to_sequences(train_texts)\n",
        "X_train = pad_sequences(encoded_docs_train, maxlen=max_length, padding='post')\n",
        "y_train=np.asarray(train_labels)\n",
        "encoded_docs_val = tokenizer.texts_to_sequences(val_texts)\n",
        "X_val = pad_sequences(encoded_docs_val, maxlen=max_length, padding='post')\n",
        "y_val=np.asarray(val_labels)"
      ],
      "metadata": {
        "id": "Sd1dQQH9TLUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b6c558-b24f-49c1-e16b-fbf72a26437d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-9-b5f281e7850a>:60: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  embedding_matrix[i,0:100]=fasttext[word]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPTNqxlUxZYe",
        "outputId": "9da56a65-7e22-4176-f1b4-49987fad36db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8452, 46), (8452,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val.shape,y_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a8KifJ3x0oe",
        "outputId": "f4837d51-5578-4186-896f-e381d5f50cd1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((940, 46), (940,))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.count(0),train_labels.count(1), train_labels.count(2), len(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS9eINUqx33F",
        "outputId": "ce353d3c-7579-42f4-e9c8-e869f2f552e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1561, 6280, 611, 8452)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train\n",
        "X_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWwfJ4wAx6aV",
        "outputId": "c3cf9c58-1ed0-4f2a-eb2b-1060511967f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   11,    60,    57, ...,     0,     0,     0],\n",
              "       [   60,    57,    17, ...,     0,     0,     0],\n",
              "       [17512,    17,  2820, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [ 3335, 18630,     8, ...,     0,     0,     0],\n",
              "       [  177,     4,    92, ...,     0,     0,     0],\n",
              "       [   11, 11254,    14, ...,     0,     0,     0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "А теперь дело за тем, чтобы создать свёрточную нейросеть. (Пока что об аугментации речь не заходит.) Будем использовать экспоненциальное снижение скорости обучения, и это, к слову, уже первый повод поиграться с параметрами."
      ],
      "metadata": {
        "id": "85MUs67-yAMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "from tensorflow.keras.layers import Flatten, Conv1D, MaxPooling1D, SpatialDropout1D,Dense,Dropout,Embedding\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.models import Sequential\n",
        "from keras.initializers import he_uniform, glorot_uniform\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
      ],
      "metadata": {
        "id": "w62mxiGGE3oW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=20000,\n",
        "    decay_rate=0.96\n",
        ")"
      ],
      "metadata": {
        "id": "i4hKHQPJFKAl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConvolutionNet = Sequential()\n",
        "ConvolutionNet.add(Embedding(len(vocab)+1, 100,weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "ConvolutionNet.add(Conv1D(64, 2, padding='valid', activation='relu',\n",
        "               kernel_initializer=he_uniform(seed=RANDOM_SEED), name='Conv_B1_L1'))\n",
        "ConvolutionNet.add(Conv1D(64, 2, activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),\n",
        "               name='Conv_B1_L2'))\n",
        "ConvolutionNet.add(MaxPooling1D(pool_size=2, name='MP1'))\n",
        "ConvolutionNet.add(SpatialDropout1D(rate=0.15, name='SD1', seed=RANDOM_SEED))\n",
        "\n",
        "ConvolutionNet.add(Conv1D(64, 3, padding='valid', activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),name='Conv_B2_L1'))\n",
        "ConvolutionNet.add(Conv1D(64, 3, activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),name='Conv_B2_L2'))\n",
        "ConvolutionNet.add(MaxPooling1D(pool_size=2, name='MP2'))\n",
        "ConvolutionNet.add(SpatialDropout1D(rate=0.15, name='SD2', seed=RANDOM_SEED))\n",
        "\n",
        "ConvolutionNet.add(Flatten())\n",
        "ConvolutionNet.add(Dense(512, activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED), name='HL'))\n",
        "ConvolutionNet.add(Dropout(rate=0.5, seed=RANDOM_SEED, name='DAH'))\n",
        "ConvolutionNet.add(Dense(3, activation='softmax', kernel_initializer=glorot_uniform(seed=RANDOM_SEED), name='OL'))\n",
        "ConvolutionNet.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
        "ConvolutionNet.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnos6DIHLsaU",
        "outputId": "7b466dff-8e7f-44cc-b472-c32c5f2b19a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 46, 100)           2165600   \n",
            "                                                                 \n",
            " Conv_B1_L1 (Conv1D)         (None, 45, 64)            12864     \n",
            "                                                                 \n",
            " Conv_B1_L2 (Conv1D)         (None, 44, 64)            8256      \n",
            "                                                                 \n",
            " MP1 (MaxPooling1D)          (None, 22, 64)            0         \n",
            "                                                                 \n",
            " SD1 (SpatialDropout1D)      (None, 22, 64)            0         \n",
            "                                                                 \n",
            " Conv_B2_L1 (Conv1D)         (None, 20, 64)            12352     \n",
            "                                                                 \n",
            " Conv_B2_L2 (Conv1D)         (None, 18, 64)            12352     \n",
            "                                                                 \n",
            " MP2 (MaxPooling1D)          (None, 9, 64)             0         \n",
            "                                                                 \n",
            " SD2 (SpatialDropout1D)      (None, 9, 64)             0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 576)               0         \n",
            "                                                                 \n",
            " HL (Dense)                  (None, 512)               295424    \n",
            "                                                                 \n",
            " DAH (Dropout)               (None, 512)               0         \n",
            "                                                                 \n",
            " OL (Dense)                  (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,508,387\n",
            "Trainable params: 342,787\n",
            "Non-trainable params: 2,165,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ConvolutionNet.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val),\n",
        "    shuffle=True, epochs=75,\n",
        "    callbacks=[\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss', patience=7, restore_best_weights=True, verbose=1\n",
        "        )\n",
        "    ],\n",
        "    verbose=1)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV7tNMmPPOia",
        "outputId": "db07e658-13f7-4956-87e5-5a6cc055bb68"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "133/133 [==============================] - 9s 70ms/step - loss: 0.4604 - sparse_categorical_accuracy: 0.8174 - val_loss: 0.4689 - val_sparse_categorical_accuracy: 0.7936\n",
            "Epoch 2/75\n",
            "133/133 [==============================] - 6s 42ms/step - loss: 0.4271 - sparse_categorical_accuracy: 0.8319 - val_loss: 0.4538 - val_sparse_categorical_accuracy: 0.8021\n",
            "Epoch 3/75\n",
            "133/133 [==============================] - 9s 66ms/step - loss: 0.3870 - sparse_categorical_accuracy: 0.8454 - val_loss: 0.5076 - val_sparse_categorical_accuracy: 0.7872\n",
            "Epoch 4/75\n",
            "133/133 [==============================] - 9s 65ms/step - loss: 0.3673 - sparse_categorical_accuracy: 0.8560 - val_loss: 0.4300 - val_sparse_categorical_accuracy: 0.8255\n",
            "Epoch 5/75\n",
            "133/133 [==============================] - 6s 46ms/step - loss: 0.3292 - sparse_categorical_accuracy: 0.8697 - val_loss: 0.4243 - val_sparse_categorical_accuracy: 0.8277\n",
            "Epoch 6/75\n",
            "133/133 [==============================] - 8s 57ms/step - loss: 0.3146 - sparse_categorical_accuracy: 0.8768 - val_loss: 0.4532 - val_sparse_categorical_accuracy: 0.8245\n",
            "Epoch 7/75\n",
            "133/133 [==============================] - 8s 56ms/step - loss: 0.2881 - sparse_categorical_accuracy: 0.8895 - val_loss: 0.4531 - val_sparse_categorical_accuracy: 0.8191\n",
            "Epoch 8/75\n",
            "133/133 [==============================] - 7s 56ms/step - loss: 0.2587 - sparse_categorical_accuracy: 0.9024 - val_loss: 0.4735 - val_sparse_categorical_accuracy: 0.8309\n",
            "Epoch 9/75\n",
            "133/133 [==============================] - 8s 60ms/step - loss: 0.2345 - sparse_categorical_accuracy: 0.9087 - val_loss: 0.4521 - val_sparse_categorical_accuracy: 0.8330\n",
            "Epoch 10/75\n",
            "133/133 [==============================] - 9s 65ms/step - loss: 0.2167 - sparse_categorical_accuracy: 0.9193 - val_loss: 0.5145 - val_sparse_categorical_accuracy: 0.8277\n",
            "Epoch 11/75\n",
            "133/133 [==============================] - 8s 57ms/step - loss: 0.1942 - sparse_categorical_accuracy: 0.9303 - val_loss: 0.5765 - val_sparse_categorical_accuracy: 0.8277\n",
            "Epoch 12/75\n",
            "132/133 [============================>.] - ETA: 0s - loss: 0.1723 - sparse_categorical_accuracy: 0.9384Restoring model weights from the end of the best epoch: 5.\n",
            "133/133 [==============================] - 8s 58ms/step - loss: 0.1724 - sparse_categorical_accuracy: 0.9384 - val_loss: 0.4726 - val_sparse_categorical_accuracy: 0.8372\n",
            "Epoch 12: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1b704ed90>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим не самую худшую точность, однако оценим качество, как полагается, вдруг там есть подводные камни?"
      ],
      "metadata": {
        "id": "XDbTFrd1Q_y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mokoron/sentirueval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAczzhPFVM0p",
        "outputId": "2534cfd9-3d80-4d2e-fc99-669ae29e6f35"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sentirueval'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 28 (delta 10), reused 12 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts,test_labels=load_sentirueval_2016('/content/sentirueval/banks_test_etalon.xml')\n",
        "test_num_labels=[label_to_num[label] for label in test_labels]\n",
        "encoded_docs_test = tokenizer.texts_to_sequences(test_texts)\n",
        "X_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
        "y_test=np.asarray(test_num_labels)\n",
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBc2Cn9oQ-BS",
        "outputId": "a3e42dc7-e920-410c-a372-c1bbdd6a5991"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 64,   4,  25, ...,   0,   0,   0],\n",
              "       [ 11,   2,   1, ...,   0,   0,   0],\n",
              "       [ 64,   4,  25, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [  8,   7,  12, ...,   0,   0,   0],\n",
              "       [ 60,  57, 173, ...,   0,   0,   0],\n",
              "       [ 60,  57,  89, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(ConvolutionNet.predict(X_test, batch_size=128), axis=1)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, target_names=label_to_num.keys(), digits=4))\n",
        "from sklearn.metrics import f1_score\n",
        "macro=f1_score(test_num_labels,y_pred,average='macro',labels=[0,2])\n",
        "micro=f1_score(test_num_labels,y_pred,average='micro',labels=[0,2])\n",
        "macro,micro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn8KvKCYVvE5",
        "outputId": "642ef2e5-bc0b-4e33-b3f9-ebf886693d49"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/26 [==============================] - 1s 24ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.5408    0.4316    0.4801       767\n",
            "     neutral     0.7491    0.8298    0.7874      2238\n",
            "    positive     0.1892    0.1364    0.1585       308\n",
            "\n",
            "    accuracy                         0.6731      3313\n",
            "   macro avg     0.4930    0.4659    0.4753      3313\n",
            "weighted avg     0.6488    0.6731    0.6578      3313\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3192742895453364, 0.3907805133577789)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ну что сказать - попробуем спасти ситуацию. Аугментацией, конечно. Есть два способа - опечатками и синонимами. Естественно, нужно попробовать оба."
      ],
      "metadata": {
        "id": "94f3wUTbWHAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textattack\n",
        "!pip install tensorflow_text\n",
        "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
        "from textattack.transformations import WordSwapQWERTY\n",
        "from textattack.transformations import CompositeTransformation\n",
        "from textattack.constraints.pre_transformation import RepeatModification\n",
        "from textattack.constraints.pre_transformation import StopwordModification\n",
        "from textattack.augmentation import Augmenter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT27hZf0WWnw",
        "outputId": "427b9ad8-51d0-444e-82ec-497bce160e6b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textattack\n",
            "  Downloading textattack-0.3.8-py3-none-any.whl (418 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.7/418.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.13.0+cu116)\n",
            "Collecting lru-dict\n",
            "  Downloading lru_dict-1.1.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from textattack) (4.64.1)\n",
            "Collecting flair\n",
            "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.9/401.9 KB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from textattack) (9.0.0)\n",
            "Collecting OpenHowNet\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.8/dist-packages (from textattack) (7.1.2)\n",
            "Collecting bert-score>=0.3.5\n",
            "  Downloading bert_score-0.3.12-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.21.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from textattack) (3.9.0)\n",
            "Collecting lemminflect\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 KB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.8/dist-packages (from textattack) (0.42.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.3.5)\n",
            "Collecting pinyin==0.4.0\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from textattack) (3.7)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.7.3)\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycld2\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.8/dist-packages (from textattack) (0.5.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from textattack) (1.7.1)\n",
            "Collecting datasets==2.4.0\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 KB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (2022.11.0)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (21.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (3.8.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->textattack) (2.25.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from bert-score>=0.3.5->textattack) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.1->textattack) (2022.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.21.0->textattack) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.21.0->textattack) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (1.0.2)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 KB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (4.9.2)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (4.4.0)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Collecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (0.8.10)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from flair->textattack) (3.6.0)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair->textattack) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair->textattack) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->textattack) (1.2.0)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from OpenHowNet->textattack) (57.4.0)\n",
            "Collecting anytree\n",
            "  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.4->flair->textattack) (1.14.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.3.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair->textattack) (6.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair->textattack) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair->textattack) (2.2.0)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair->textattack) (2.8.8)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2022.12.7)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (3.1.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->flair->textattack) (0.2.5)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair->textattack) (3.11.0)\n",
            "Building wheels for collected packages: pinyin, mpld3, pycld2, word2number, docopt, sqlitedict, langdetect, pptree, overrides\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630495 sha256=ce0df1deb8e48e2e75a1d16090799006c8097bb9f750b84f2e0fe88c66d7c45b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/2a/d9/9c0f787a4d55f9a9eca26d322eafbe083bab41cb9bffb2e6e8\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=2b98cb5b6e92f9b75ecdc1bb92ebfc1758b8ded30891cfb43d8151f2688cc3f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/9f/9d/d806a20bd97bc7076d724fa3e69fa5be61836ba16b2ffa6126\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp38-cp38-linux_x86_64.whl size=9833571 sha256=82f7f8dce6917cb1c17cebe71bdbc9621ad09cbe5f5dad9fac541e306e5765ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/3a/82/d990040cbe6c3527732e931e2925785e83fe9aaa5a11c313ca\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=ffaa23f54d5ecc8ba713dcc8aa8c1f57de1f574e273dba5a316be0fd3e2848fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/f3/5a/d88198fdeb46781ddd7e7f2653061af83e7adb2a076d8886d6\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=533e1d4c01553b7ab9445879e4358e7141b59abc3f6d6b398cbfcfca4bac6f4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16869 sha256=a6f57cc1d65dd9da6a47590f39b188f786d5c4283230eeebde8a2818dd03d69d\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/c6/16/46e174009277f9bccdaa7215a243939d2f70180804b249bf3a\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=e4d1394ca864f4ebf55d4ad449012171ce2834e7bc62342fd8e98a982824358c\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=3046ee7a2ac2969a10df37cc872be2f73a5bf0b679d5bf9e6fd9c2fc71aef597\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/8b/30/5b20240d3d13a9dfafb6a6dd49d1b541c86d39812cb3690edf\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=9c53e7542155d55100f1461cf5306ea5204670a87a5fbaf2e6aac2009d22be8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
            "Successfully built pinyin mpld3 pycld2 word2number docopt sqlitedict langdetect pptree overrides\n",
            "Installing collected packages: word2number, tokenizers, sqlitedict, sentencepiece, pycld2, py4j, pptree, pinyin, overrides, mpld3, lru-dict, janome, docopt, xxhash, urllib3, terminaltables, segtok, num2words, lemminflect, langdetect, importlib-metadata, ftfy, dill, deprecated, conllu, anytree, multiprocess, hyperopt, wikipedia-api, responses, OpenHowNet, language-tool-python, konoha, huggingface-hub, bpemb, transformers, datasets, flair, bert-score, textattack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenHowNet-2.0 anytree-2.8.0 bert-score-0.3.12 bpemb-0.3.4 conllu-4.5.2 datasets-2.4.0 deprecated-1.2.13 dill-0.3.5.1 docopt-0.6.2 flair-0.11.3 ftfy-6.1.1 huggingface-hub-0.11.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.1.8 mpld3-0.3 multiprocess-0.70.13 num2words-0.5.12 overrides-3.1.0 pinyin-0.4.0 pptree-3.1 py4j-0.10.9.7 pycld2-0.41 responses-0.18.0 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.8 tokenizers-0.13.2 transformers-4.25.1 urllib3-1.26.14 wikipedia-api-0.5.8 word2number-1.1 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Collecting tensorflow<2.12,>=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.29.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.19.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.15.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.21.6)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.4-py2.py3-none-any.whl (26 kB)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (14.0.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.51.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (4.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.15.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.1)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.2.2)\n",
            "Installing collected packages: flatbuffers, tensorflow-estimator, keras, importlib-metadata, tensorboard, tensorflow, tensorflow_text\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 3.10.1\n",
            "    Uninstalling importlib-metadata-3.10.1:\n",
            "      Successfully uninstalled importlib-metadata-3.10.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "konoha 4.6.5 requires importlib-metadata<4.0.0,>=3.7.0, but you have importlib-metadata 6.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-23.1.4 importlib-metadata-6.0.0 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow_text-2.11.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Updating TextAttack package dependencies.\n",
            "textattack: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Ochepyatka(WordSwapQWERTY):\n",
        "  def __init__(\n",
        "        self, random_one=True, skip_first_char=True, skip_last_char=True, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.random_one = random_one\n",
        "        self.skip_first_char = skip_first_char\n",
        "        self.skip_last_char = skip_last_char\n",
        "\n",
        "        self._keyboard_adjacency = {\n",
        "            \"ё\":[\"е\"],\n",
        "            \"й\": [\"ц\",\"ф\",\"ё\"],\n",
        "            \"ц\": [\"й\", \"у\", \"ф\", \"ы\", \"в\"],\n",
        "            \"у\": [\"ц\", \"ы\", \"в\", \"а\", \"к\"],\n",
        "            \"к\": [\"у\", \"в\", \"а\", \"п\", \"е\"],\n",
        "            \"е\": [\"к\", \"а\", \"п\", \"р\", \"н\"],\n",
        "            \"н\": [\"е\", \"п\", \"р\", \"о\", \"г\"],\n",
        "            \"г\": [\"н\", \"р\", \"о\", \"л\", \"ш\"],\n",
        "            \"ш\": [\"г\", \"о\", \"л\", \"д\", \"щ\"],\n",
        "            \"щ\": [\"ш\", \"л\", \"д\", \"з\"],\n",
        "            \"з\": [\"щ\", \"д\",\"ж\",\"э\",\"х\"],\n",
        "            \"х\": [\"з\", \"ж\",\"э\",\"ъ\"],\n",
        "            \"ъ\": [\"х\", \"ж\",\"э\"],\n",
        "            \"ф\": [\"й\", \"ц\", \"ы\", \"я\", \"ч\"],\n",
        "            \"ы\": [\"й\", \"ц\", \"у\", \"а\", \"в\", \"я\", \"ч\"],\n",
        "            \"в\": [\"ц\", \"у\", \"к\", \"а\", \"с\", \"ч\", \"ы\"],\n",
        "            \"а\": [\"у\", \"к\", \"е\", \"п\", \"м\", \"с\", \"в\"],\n",
        "            \"п\": [\"к\", \"е\", \"н\", \"р\", \"и\", \"м\", \"в\"],\n",
        "            \"р\": [\"е\", \"н\", \"г\", \"п\", \"о\", \"и\", \"т\"],\n",
        "            \"о\": [\"н\", \"г\", \"ш\", \"л\", \"ь\", \"т\", \"р\"],\n",
        "            \"л\": [\"г\", \"ш\", \"щ\", \"д\", \"ь\", \"о\"],\n",
        "            \"д\": [\"ш\", \"щ\", \"з\", \"л\",\"ж\",\"ю\",\"б\"],\n",
        "            \"ж\": [\"д\", \"щ\", \"з\", \"х\",\"э\",\"ю\"],\n",
        "            \"э\": [\"ж\", \"з\", \"х\", \"ъ\",\"ю\"],\n",
        "            \"я\": [\"ф\", \"ы\", \"ч\"],\n",
        "            \"ч\": [\"ы\", \"в\", \"я\", \"с\"],\n",
        "            \"с\": [\"ч\", \"в\", \"а\", \"м\"],\n",
        "            \"м\": [\"с\", \"а\", \"п\", \"и\"],\n",
        "            \"и\": [\"м\", \"п\", \"р\", \"т\"],\n",
        "            \"т\": [\"и\", \"р\", \"о\", \"ь\"],\n",
        "            \"ь\": [\"т\", \"о\", \"л\",\"б\"],\n",
        "            \"б\": [\"ь\", \"л\", \"д\",\"ю\"],\n",
        "            \"ю\": [\"б\", \"д\", \"ж\"]\n",
        "        }\n",
        "  def _get_adjacent(self, s):\n",
        "        s_lower = s.lower()\n",
        "        if s_lower in self._keyboard_adjacency:\n",
        "            adjacent_keys = self._keyboard_adjacency[s_lower]\n",
        "            if s.isupper():\n",
        "                return [key.upper() for key in adjacent_keys]\n",
        "            else:\n",
        "                return adjacent_keys\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "  def _get_replacement_words(self, word):\n",
        "        if len(word) <= 1:\n",
        "            return []\n",
        "\n",
        "        candidate_words = []\n",
        "\n",
        "        start_idx = 1 if self.skip_first_char else 0\n",
        "        end_idx = len(word) - (1 + self.skip_last_char)\n",
        "\n",
        "        if start_idx >= end_idx:\n",
        "            return []\n",
        "\n",
        "        if self.random_one:\n",
        "            i = random.randrange(start_idx, end_idx + 1)\n",
        "            candidate_word = (\n",
        "                word[:i] + random.choice(self._get_adjacent(word[i])) + word[i + 1 :]\n",
        "            )\n",
        "            candidate_words.append(candidate_word)\n",
        "        else:\n",
        "            for i in range(start_idx, end_idx + 1):\n",
        "                for swap_key in self._get_adjacent(word[i]):\n",
        "                    candidate_word = word[:i] + swap_key + word[i + 1 :]\n",
        "                    candidate_words.append(candidate_word)\n",
        "\n",
        "        return candidate_words"
      ],
      "metadata": {
        "id": "ZpdXXccpY59_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "typo_transformation = CompositeTransformation([WordSwapRandomCharacterDeletion(),Ochepyatka(random_one=False)])\n",
        "typo_constraints = [RepeatModification(),StopwordModification(language='russian')]\n",
        "typo_augmenter = Augmenter(transformation=typo_transformation, constraints=typo_constraints, pct_words_to_swap=0.5, transformations_per_example=5)\n",
        "s = texts[1]\n",
        "typo_augmenter.augment(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7wd-IARaxZp",
        "outputId": "288ae204-dba3-4c4d-bbf2-16f19a2b910c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Мгение о крежитной курте втб 2 http://t.c/SBJTcsqjCg',\n",
              " 'Мнегие о коедитной карье втб 24 http://t.o/SBJTcqjCg',\n",
              " 'Мнение о кредттной карре втб 2 htp://t.co/SBJTcsqjC',\n",
              " 'Мнепие о креюитной карье втб 2 http://t.co/SBJTcsjCg',\n",
              " 'Мнкние о кредитнрй катте втб 2 http://t.o/SBJTcsqjCg']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tqdm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3pIF-ggbKh5",
        "outputId": "01fac2cf-635a-4037-c0f1-20a0815b898a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts_with_typos=[]\n",
        "labels_of_texts_with_typos=[]\n",
        "for indx,text in enumerate(tqdm(train_texts)):\n",
        "    texts_with_typos.extend(typo_augmenter.augment(text))\n",
        "    labels_of_texts_with_typos.extend([train_labels[indx]]*5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYMHeUG-bCLY",
        "outputId": "9c70817b-ef67-42b9-cd57-25a678ffe167"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8452/8452 [1:35:14<00:00,  1.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ochepyatki.csv', 'w', encoding='utf-8') as f1:\n",
        "    writer = csv.DictWriter(f1,fieldnames=['text','label'])\n",
        "    writer.writeheader()\n",
        "    for i,text in enumerate(texts):\n",
        "        writer.writerow({'text':text,'label':labels_of_texts_with_typos[i]})\n",
        "\n",
        "print(len(labels_of_texts_with_typos),len(texts_with_typos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYwa1vWfxnRR",
        "outputId": "5526b3e4-4119-44ec-b0b7-34abeb799d8d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42260 42260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ochepyatki.csv', 'r', encoding='utf-8') as file:\n",
        "  texts_with_typos=[]\n",
        "  labels_of_texts_with_typos=[]\n",
        "  reader = csv.DictReader(file)\n",
        "  for row in reader:\n",
        "    if row!='':\n",
        "      texts_with_typos.append(row['text'])\n",
        "      labels_of_texts_with_typos.append(int(row['label']))"
      ],
      "metadata": {
        "id": "PFpIlF_Y2_4R"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ochepyatki=[]\n",
        "pos_ochepyatki_labels=[]\n",
        "for text_idx,text in enumerate(texts_with_typos):\n",
        "  if labels_of_texts_with_typos[text_idx]==2:\n",
        "    pos_ochepyatki.append(text)\n",
        "    pos_ochepyatki_labels.append(labels_of_texts_with_typos[text_idx])\n",
        "\n",
        "neg_ochepyatki=[]\n",
        "neg_ochepyatki_labels=[]\n",
        "for text_idx,text in enumerate(texts_with_typos):\n",
        "  if labels_of_texts_with_typos[text_idx]==0:\n",
        "    neg_ochepyatki.append(text)\n",
        "    neg_ochepyatki_labels.append(labels_of_texts_with_typos[text_idx])\n",
        "\n",
        "pos_neg_ochep=[]\n",
        "pos_neg_ochep_labels=[]\n",
        "for text_idx,text in enumerate(texts_with_typos):\n",
        "  if labels_of_texts_with_typos[text_idx]!=1:\n",
        "    pos_neg_ochep.append(text)\n",
        "    pos_neg_ochep_labels.append(labels_of_texts_with_typos[text_idx])"
      ],
      "metadata": {
        "id": "snvkZ_IdxwU5"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если с опечатками мы вывернулись, надстроив удобный нам класс из имеющихся, то с синонимами так не получится - придётся брать из ворднета."
      ],
      "metadata": {
        "id": "zbuMIt8O0Ewh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy_udpipe\n",
        "!pip install wiki_ru_wordnet\n",
        "import spacy_udpipe\n",
        "spacy_udpipe.download(\"ru\") \n",
        "nlp = spacy_udpipe.load(\"ru\")\n",
        "from wiki_ru_wordnet import WikiWordnet\n",
        "wikiwordnet = WikiWordnet()\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "rus_stopwords=stopwords.words('russian')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULCjHddr0qpg",
        "outputId": "c8e62345-4dfe-46d8-c387-8b2d4a0aa322"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy_udpipe in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy_udpipe) (3.4.4)\n",
            "Requirement already satisfied: ufal.udpipe>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy_udpipe) (1.2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.10.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (6.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.25.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.1.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.11)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wiki_ru_wordnet in /usr/local/lib/python3.8/dist-packages (1.0.3)\n",
            "Already downloaded a model for the 'ru' language\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def total_syn_aug(text,n:int)->List[str]:\n",
        "  sourc=nlp(text.lower())\n",
        "  lemmatext=[wf.lemma_ for wf in sourc]\n",
        "  result=[]\n",
        "  for i in range(n):\n",
        "    changed_text=[]\n",
        "    for word in lemmatext:\n",
        "      if word not in rus_stopwords:\n",
        "        synsets = wikiwordnet.get_synsets(word)\n",
        "        if synsets:\n",
        "          synset=random.choice(synsets)\n",
        "          words=synset.get_words()\n",
        "          new_word=random.choice(list(words))\n",
        "          if new_word.lemma()!=word:\n",
        "            changed_text.append(new_word.lemma())\n",
        "        else:\n",
        "         changed_text.append(word)\n",
        "      else:\n",
        "        changed_text.append(word)\n",
        "    if changed_text!=lemmatext:\n",
        "      result.append(' '.join(changed_text))\n",
        "  return set(result)"
      ],
      "metadata": {
        "id": "T_FQoeKP3TMg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_with_synonyms=[]\n",
        "labels_of_texts_with_synonyms=[]\n",
        "for text_idx,text in enumerate(tqdm(train_texts)):\n",
        "  if train_labels[text_idx]==2:\n",
        "    augmented_texts=total_syn_aug(text,5)\n",
        "    texts_with_synonyms.extend(augmented_texts)\n",
        "    labels_of_texts_with_synonyms.extend([train_labels[text_idx]]*len(augmented_texts))\n",
        "  '''elif train_labels[text_idx]==0:\n",
        "    augmented_texts=augment_with_synonyms(text,3)\n",
        "    texts_with_synonyms.extend(augmented_texts)\n",
        "    labels_of_texts_with_synonyms.extend([train_labels[text_idx]]*len(augmented_texts))'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwLNORWs4TYA",
        "outputId": "2f41a404-df8b-4c70-d244-db094365c0ff"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8452/8452 [00:14<00:00, 583.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts_with_synonyms),len(labels_of_texts_with_synonyms))\n",
        "texts_with_synonyms[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k52CDeEh4l5Y",
        "outputId": "a4a4e217-06bf-4e79-bbe5-28cb2e974554"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1991 1991\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['и втб24 поднимать процент по . госбанка догонять : 23 повысить по картина - -',\n",
              " 'и втб24 процент по . госбанка догонять : 23 повысить по сорт - -',\n",
              " 'и втб24 поднимать процент по . госбанка догонять : 23 повысить по - -',\n",
              " 'и втб24 процент по . госбанка догонять торг : 23 повысить по картина - -',\n",
              " 'обновить r mobile от @raiffeisen_ru и больно . в предшествующий не , но сейчас без хула .',\n",
              " 'обновить r mobile от @raiffeisen_ru и зело . в прошлый не , но сейчас без порицание .',\n",
              " 'обновить r mobile от @raiffeisen_ru и чересчур . в прошлый редакция не доставать , но сейчас без укор .',\n",
              " 'обновить r mobile от @raiffeisen_ru и чрезвычайно пьяный . в прошлый не , но сейчас без укор .',\n",
              " 'обновить r mobile от @raiffeisen_ru и крайне . в не , но сейчас без .',\n",
              " 'да ! я же не ! пенсия я быть с сбербанок .',\n",
              " 'да ! я же не ! пенсия я быть накоплять с сбербанок .',\n",
              " 'россия обновить сайт http://t.co/343h6e1cek',\n",
              " 'россия обновить http://t.co/343h6e1cek',\n",
              " 'обновить : http://t.co/eis9wiw7ls',\n",
              " '# россия – финансовый : на россия . . . http://t.co/q01bjytiis #',\n",
              " '# россия – финансовый торг : на неурезанный россия . . . http://t.co/q01bjytiis #',\n",
              " '# россия – босс финансовый : на россия . . . http://t.co/q01bjytiis #',\n",
              " '# россия – глава финансовый : на россия . . . http://t.co/q01bjytiis #',\n",
              " 'rt @running_miss : последний сбербанок максимально надобность юзер',\n",
              " 'rt @running_miss : недавний веб-сайт сбербанок максимально юзер']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Не такой хороший буст, как от опечаток, но тоже ничего. Далее планировалось включить аугментацию с помощью берта, но произошло то, чего, возможно, следовало ожидать (хотя я не ожидал): даже несущественный результат требовал нескольких дней ожидания. Ну а теперь проверка качества - перетусовка на фолды, расширение нашими синонимами и само обучение."
      ],
      "metadata": {
        "id": "EzCanv_14pdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, num_labels, test_size=0.1, random_state=RANDOM_SEED)\n",
        "train_texts.extend(texts_with_synonyms)\n",
        "train_labels.extend(labels_of_texts_with_synonyms)\n",
        "train_texts.extend(pos_neg_ochep)\n",
        "train_labels.extend(pos_neg_ochep_labels)\n",
        "train_labels.count(0),train_labels.count(1), train_labels.count(2), len(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJaflks74uTI",
        "outputId": "b9613263-8859-4096-d771-4a6ad5ded24c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3211, 6280, 3347, 8452)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_docs_train = tokenizer.texts_to_sequences(train_texts)\n",
        "X_train = pad_sequences(encoded_docs_train, maxlen=max_length, padding='post')\n",
        "y_train=np.asarray(train_labels)"
      ],
      "metadata": {
        "id": "gWCT4_LE6uln"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Convolution2 = Sequential()\n",
        "Convolution2.add(Embedding(len(vocab)+1, 100,weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "Convolution2.add(Conv1D(64, 3, padding='valid', activation='relu',\n",
        "               kernel_initializer=he_uniform(seed=RANDOM_SEED), name='Conv_B1_L1'))\n",
        "Convolution2.add(Conv1D(64, 3, activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),\n",
        "               name='Conv_B1_L2'))\n",
        "Convolution2.add(MaxPooling1D(pool_size=2, name='MP1'))\n",
        "Convolution2.add(SpatialDropout1D(rate=0.15, name='SD1', seed=RANDOM_SEED))\n",
        "\n",
        "Convolution2.add(Conv1D(64, 4, padding='valid', activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),name='Conv_B2_L1'))\n",
        "Convolution2.add(Conv1D(64, 4, activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),name='Conv_B2_L2'))\n",
        "Convolution2.add(MaxPooling1D(pool_size=2, name='MP2'))\n",
        "Convolution2.add(SpatialDropout1D(rate=0.15, name='SD2', seed=RANDOM_SEED))\n",
        "\n",
        "Convolution2.add(Flatten())\n",
        "Convolution2.add(Dense(512, activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED), name='HL'))\n",
        "Convolution2.add(Dropout(rate=0.5, seed=RANDOM_SEED, name='DAH'))\n",
        "Convolution2.add(Dense(3, activation='softmax', kernel_initializer=glorot_uniform(seed=RANDOM_SEED), name='OL'))\n",
        "Convolution2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
        "Convolution2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu8Oxkxo7Alv",
        "outputId": "33cd5c84-c5db-4dd9-8ae3-238b4c766d84"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 46, 100)           2165600   \n",
            "                                                                 \n",
            " Conv_B1_L1 (Conv1D)         (None, 44, 64)            19264     \n",
            "                                                                 \n",
            " Conv_B1_L2 (Conv1D)         (None, 42, 64)            12352     \n",
            "                                                                 \n",
            " MP1 (MaxPooling1D)          (None, 21, 64)            0         \n",
            "                                                                 \n",
            " SD1 (SpatialDropout1D)      (None, 21, 64)            0         \n",
            "                                                                 \n",
            " Conv_B2_L1 (Conv1D)         (None, 18, 64)            16448     \n",
            "                                                                 \n",
            " Conv_B2_L2 (Conv1D)         (None, 15, 64)            16448     \n",
            "                                                                 \n",
            " MP2 (MaxPooling1D)          (None, 7, 64)             0         \n",
            "                                                                 \n",
            " SD2 (SpatialDropout1D)      (None, 7, 64)             0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 448)               0         \n",
            "                                                                 \n",
            " HL (Dense)                  (None, 512)               229888    \n",
            "                                                                 \n",
            " DAH (Dropout)               (None, 512)               0         \n",
            "                                                                 \n",
            " OL (Dense)                  (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,461,539\n",
            "Trainable params: 295,939\n",
            "Non-trainable params: 2,165,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Convolution2.fit(\n",
        "    X_train, \n",
        "    y_train, \n",
        "    batch_size=64, \n",
        "    validation_data=(X_val, y_val), \n",
        "    shuffle=True, \n",
        "    epochs=75, \n",
        "    callbacks=[EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)], \n",
        "    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCEC7jx8-KZf",
        "outputId": "bc2a7681-c619-4416-cf5d-2e2d38960567"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "201/201 [==============================] - 9s 39ms/step - loss: 0.9758 - sparse_categorical_accuracy: 0.5608 - val_loss: 0.7064 - val_sparse_categorical_accuracy: 0.7585\n",
            "Epoch 2/75\n",
            "201/201 [==============================] - 8s 39ms/step - loss: 0.8765 - sparse_categorical_accuracy: 0.6298 - val_loss: 0.7361 - val_sparse_categorical_accuracy: 0.7489\n",
            "Epoch 3/75\n",
            "201/201 [==============================] - 8s 39ms/step - loss: 0.8099 - sparse_categorical_accuracy: 0.6743 - val_loss: 0.7095 - val_sparse_categorical_accuracy: 0.7447\n",
            "Epoch 4/75\n",
            "201/201 [==============================] - 11s 55ms/step - loss: 0.7536 - sparse_categorical_accuracy: 0.7053 - val_loss: 0.5942 - val_sparse_categorical_accuracy: 0.7979\n",
            "Epoch 5/75\n",
            "201/201 [==============================] - 10s 51ms/step - loss: 0.7148 - sparse_categorical_accuracy: 0.7204 - val_loss: 0.5887 - val_sparse_categorical_accuracy: 0.8064\n",
            "Epoch 6/75\n",
            "201/201 [==============================] - 8s 37ms/step - loss: 0.6875 - sparse_categorical_accuracy: 0.7334 - val_loss: 0.5699 - val_sparse_categorical_accuracy: 0.8106\n",
            "Epoch 7/75\n",
            "201/201 [==============================] - 7s 36ms/step - loss: 0.6643 - sparse_categorical_accuracy: 0.7497 - val_loss: 0.6160 - val_sparse_categorical_accuracy: 0.7947\n",
            "Epoch 8/75\n",
            "201/201 [==============================] - 7s 36ms/step - loss: 0.6481 - sparse_categorical_accuracy: 0.7527 - val_loss: 0.5768 - val_sparse_categorical_accuracy: 0.8234\n",
            "Epoch 9/75\n",
            "201/201 [==============================] - 7s 36ms/step - loss: 0.6278 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.6654 - val_sparse_categorical_accuracy: 0.7904\n",
            "Epoch 10/75\n",
            "201/201 [==============================] - 7s 36ms/step - loss: 0.6105 - sparse_categorical_accuracy: 0.7719 - val_loss: 0.6228 - val_sparse_categorical_accuracy: 0.8074\n",
            "Epoch 11/75\n",
            "200/201 [============================>.] - ETA: 0s - loss: 0.6049 - sparse_categorical_accuracy: 0.7750Restoring model weights from the end of the best epoch: 6.\n",
            "201/201 [==============================] - 7s 36ms/step - loss: 0.6050 - sparse_categorical_accuracy: 0.7748 - val_loss: 0.6259 - val_sparse_categorical_accuracy: 0.7979\n",
            "Epoch 11: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1b703d670>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(Convolution2.predict(X_test, batch_size=128), axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLwCJ3ur_ave",
        "outputId": "89426aec-0dda-414d-b70a-0ae863d1f1b6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/26 [==============================] - 1s 17ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred, target_names=label_to_num.keys(), digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdJeH7UtAOM-",
        "outputId": "488c6851-21c6-4563-fe50-494102816822"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.4925    0.1721    0.2551       767\n",
            "     neutral     0.6942    0.7730    0.7315      2238\n",
            "    positive     0.0976    0.1753    0.1254       308\n",
            "\n",
            "    accuracy                         0.5783      3313\n",
            "   macro avg     0.4281    0.3735    0.3707      3313\n",
            "weighted avg     0.5921    0.5783    0.5649      3313\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macro=f1_score(test_num_labels,y_pred,average='macro',labels=[0,2])\n",
        "micro=f1_score(test_num_labels,y_pred,average='micro',labels=[0,2])\n",
        "macro,micro\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeiPLZsAASS2",
        "outputId": "c61e38db-fab6-436c-d0a6-e445ca1ec41c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.19025400191890118, 0.1962025316455696)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Интересные дела! У нас здорово просели практически все показатели. С чем это может быть связано? Возможно, замена синонимами привела к тому, что какие-то явно стилистически маркированные слова заменялись на нейтральные или по-другому окрашенные, и поэтому понизилось качество работы (ведь предыдущее задание показало, что не самой плохой идеей является просто в лоб считать стилистическую маркировку лексики). Может, если бы мы аугментировали синонимами с помощью берта, жить стало бы лучше, но лишней пары недель с постоянно включенным компьютером, увы, в запасе нет. Трудно предположить, как на такую просадку повлияли опечатки. В общем, факт остаётся фактом."
      ],
      "metadata": {
        "id": "yjza-cZ7At6f"
      }
    }
  ]
}